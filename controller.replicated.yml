version: "3.9"

secrets:
  ca.crt:
    file: ./secrets/ca.crt
  ca.key:
    file: ./secrets/ca.key
  controller.token:
    file: ./secrets/controller.token
  controller.secret:
    file: ./secrets/controller.secret
  worker.token:
    file: ./secrets/worker.token
  worker.secret:
    file: ./secrets/worker.secret

services:
  k0s:
    image: 'docker.io/k0sproject/k0s:v1.27.1-k0s.0' # docker.io/k0sproject/k0s:latest
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "2"
    restart: unless-stopped
    privileged: true
    deploy:
      mode: replicated
      replicas: 3
      update_config:
        parallelism: 1
        delay: 60s
    volumes:
      - /var/lib/k0s/etcd
    tmpfs:
      - /run
      - /var/run
    secrets:
      - ca.crt
      - ca.key
      - controller.token
      - controller.secret
      - worker.secret
    ports:
      - "6443:6443"
    environment:
      K0S_CONFIG: |-
        apiVersion: k0s.k0sproject.io/v1beta1
        kind: Cluster
        metadata:
          name: k0s
        spec:
          api:
            externalAddress: k0s
            sans:
              - 127.0.0.1
              - localhost
              - k0s
          storage:
            etcd:
              # https://etcd.io/docs/v3.3/op-guide/clustering/
              # https://etcd.io/docs/v3.1/op-guide/configuration/
              # https://serverfault.com/questions/1070303/etcd-cluster-configuration-for-kubernetes-which-one-should-be-considered
              extraArgs:
                # discovery: "$$(cat /run/secrets/ETCD_DISCOVERY)"
                listen-peer-urls: "https://0.0.0.0:2380"
                listen-client-urls: "https://0.0.0.0:2379"
                advertise-client-urls: "https://$${IP}:2379"
            type: etcd
        # Any additional configuration goes here ...
    # override default entrypoint for saving docker DNS
    entrypoint: ["/bin/bash", "-c"]
    command:
    - |
      # set custom hostname based on IP
      export IP=$$(hostname -i)
      export HOSTNAME=k0s-$$IP
      hostname $$HOSTNAME
      echo $$HOSTNAME > /etc/hostname
      echo $$IP $$HOSTNAME >> /etc/hosts
      echo $$IP k0s >> /etc/hosts

      # install dig and nslookup for debug dns
      apk add bind-tools iproute2-ss

      apk add gettext
      apk add etcd-ctl --repository=https://dl-cdn.alpinelinux.org/alpine/edge/testing

      mkdir -p /var/lib/k0s/pki/etcd
      mkdir -p /var/lib/k0s/manifests

      #ls -al /run/secrets

      echo "Copying pki files"
      cp /run/secrets/* /var/lib/k0s/pki/
      chown root:root -R /var/lib/k0s/pki/
      chmod 600 /var/lib/k0s/pki/ca.key /var/lib/k0s/pki/*.token
      ln -s /var/lib/k0s/pki/ca.key /var/lib/k0s/pki/etcd/ca.key
      ln -s /var/lib/k0s/pki/ca.crt /var/lib/k0s/pki/etcd/ca.crt

      echo "Copying manifests"
      cp /run/secrets/*.secret /var/lib/k0s/manifests/
      chown root:root -R /var/lib/k0s/manifests/

      # check: should we create ETCD cluster or this is a single node
      if [[ "$$(host k0s | wc -l)" -gt 1 ]]; then
        # https://etcd.io/docs/v3.3/op-guide/clustering/#public-etcd-discovery-service
        # export ETCD_DISCOVERY=$$(cat /run/secrets/ETCD_DISCOVERY)
        MEMBERS=""
        for i in $$(host k0s | awk '{print $$4}'); do
            node="k0s-$$i"
            MEMBERS+="$$node=https://$$i:2380,"
        done

        export ETCD_INITIAL_CLUSTER_TOKEN: k0s-etcd-cluster
        export ETCD_INITIAL_CLUSTER=$${MEMBERS%?}

        # TODO: check with etcdctl if cluster already exists
        # however node joined into cluster after restart even with value 'new'
        export ETCD_INITIAL_CLUSTER_STATE=new # existing

      fi

      if [ ! -z "$$K0S_CONFIG" ]; then
        echo "Creating /etc/k0s/config.yaml"
        mkdir -p /etc/k0s
        echo -n "$$K0S_CONFIG" | envsubst > /etc/k0s/config.yaml
      fi

      k0s controller --config=/etc/k0s/config.yaml --token-file /var/lib/k0s/pki/controller.token --enable-metrics-scraper
    # https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck
    # https://docs.docker.com/engine/reference/builder/#healthcheck
    healthcheck:
      test: |
        # kube-apiserver
        curl -sk https://127.0.0.1:6443 | grep -q Status || exit 1
        # konnectivity
        curl -sk http://127.0.0.1:8092 | grep -q 404 || exit 1
        # k0s
        curl -sk https://127.0.0.1:9443 | grep -q 404 || exit 1
        # kube-scheduler
        curl -sk https://127.0.0.1:10259 | grep -q Status || exit 1
        # kube-controller
        curl -sk https://127.0.0.1:10257 | grep -q Status || exit 1
        # etcd
        curl -s --cacert /var/lib/k0s/pki/etcd/ca.crt \
        --key /var/lib/k0s/pki/etcd/server.key \
        --cert /var/lib/k0s/pki/etcd/server.crt \
        https://127.0.0.1:2379/health | grep -q '"health":"true"' || exit 1

        # https://kubernetes.io/docs/reference/using-api/health-checks/
        # this command check health of whole cluster and etcd
        # in case of etcd cluster fail it returns 'livez check failed'
        # even for worked node
        #k0s kubectl get --raw='/livez?verbose' | grep -q 'livez check passed' || exit 1
      # interval in which the healthcheck process will execute
      interval: 5s
      # time duration to wait for a healthcheck
      timeout: 5s
      # define the number of tries to implement the health check after failure
      retries: 3
      # initialization time for containers that need time to bootstrap
      # it should be increased if etcd will need time to sync big amount of data
      start_period: 70s
