version: "3.9"

secrets:
  ca.crt:
    file: ./secrets/ca.crt
  ca.key:
    file: ./secrets/ca.key
  sa.pub:
    file: ./secrets/sa.pub
  sa.key:
    file: ./secrets/sa.key
  controller.token:
    file: ./secrets/controller.token
  secret_controller.yaml:
    file: ./secrets/secret_controller.yaml
  worker.token:
    file: ./secrets/worker.token
  secret_worker.yaml:
    file: ./secrets/secret_worker.yaml

x-service: &service
    image: 'docker.io/k0sproject/k0s:v1.27.1-k0s.0' # docker.io/k0sproject/k0s:latest
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "2"
    restart: unless-stopped
    privileged: true
    deploy:
      mode: replicated
      replicas: 1
      update_config:
        parallelism: 1
        #delay: 60s
    tmpfs:
      - /run
      #- /var/run
    secrets:
      - ca.crt
      - ca.key
      - sa.pub
      - sa.key
      - controller.token
      - secret_controller.yaml
      - secret_worker.yaml
    #ports:
      #- "6443:6443"
    #network_mode: "bridge"
    environment:
      K0S_CONFIG: |-
        apiVersion: k0s.k0sproject.io/v1beta1
        kind: Cluster
        metadata:
          name: k0s
        spec:
          api:
            externalAddress: k0s # external LB for cluster, should be also in sans. Can't be in loopback range
            sans:
              - 127.0.0.1
              - localhost
              - kubernetes
              - k0s
              - k0s-1
              - k0s-2
              - k0s-3
              - $$IP
              - $$IP_K0S1
              - $$IP_K0S2
              - $$IP_K0S3
          storage:
            etcd:
              # https://etcd.io/docs/v3.3/op-guide/clustering/
              # https://etcd.io/docs/v3.1/op-guide/configuration/
              # https://serverfault.com/questions/1070303/etcd-cluster-configuration-for-kubernetes-which-one-should-be-considered
              extraArgs:
                listen-peer-urls: "https://0.0.0.0:2380"
                listen-client-urls: "https://0.0.0.0:2379"
                advertise-client-urls: "https://$${IP}:2379"
                initial-advertise-peer-urls: "https://$$HOSTNAME:2380,https://$$IP:2380"
            type: etcd
          network:
            #provider: $${NETWORK_PROVIDER} # calico, kuberouter or custom
            podCIDR: 10.244.0.0/16
            serviceCIDR: 10.96.0.0/12
            calico:
              mode: vxlan
              vxlanPort: 4789
              vxlanVNI: 4096
              mtu: 1450
              wireguard: false
              flexVolumeDriverPath: /usr/libexec/k0s/kubelet-plugins/volume/exec/nodeagent~uds
              withWindowsNodes: false
              overlay: Always
            kuberouter:
              autoMTU: true
              mtu: 0
              peerRouterASNs: ""
              peerRouterIPs: ""
            kubeProxy:
              mode: iptables # iptables, ipvs, userspace
          #extensions:
            #helm:
              #concurrencyLevel: 5
              #repositories:
                #- name: stable
                  #url: https://charts.helm.sh/stable
                #- name: projectcalico
                  #url: https://docs.tigera.io/calico/charts
              #charts:
                # https://docs.tigera.io/calico/latest/getting-started/kubernetes/helm
                #- name: calico
                  #chartname: projectcalico/tigera-operator
                  #version: "v3.25.1"
                  #order: 0
                  #values: |
                    #installation:
                      #enabled: true
                      #calicoNetwork:
                        #bgp: Disabled
                        #ipPools:
                          #- cidr: 10.244.0.0/16
                            #encapsulation: VXLAN
                  #namespace: kube-system
        # Any additional configuration goes here ...
    # override default entrypoint for saving docker DNS
    entrypoint: ["/bin/bash", "-c"]
    command:
    - |
      export IP=$$(hostname -i)
      export HOSTNAME=$$(hostname)
      echo $$IP k0s kubernetes >> /etc/hosts

      # install dig and nslookup for debug dns
      apk add bind-tools iproute2-ss

      apk add gettext
      apk add etcd-ctl --repository=https://dl-cdn.alpinelinux.org/alpine/edge/testing

      #if [[ ! -f /usr/bin/calicoctl ]]; then
        #curl -sL https://github.com/projectcalico/calico/releases/latest/download/calicoctl-linux-amd64 -o /usr/bin/calicoctl
        #ln -s /usr/bin/calicoctl /usr/bin/kubectl-calico
        #chmod +x /usr/bin/calicoctl
      #fi

      #NETWORK_PROVIDER=calico
      #if calicoctl node checksystem 2>&1 | grep -q "System doesn't meet"
      #then
        #NETWORK_PROVIDER=kuberouter
      #fi
      #export NETWORK_PROVIDER=$$NETWORK_PROVIDER

      mkdir -p /var/lib/k0s/pki/etcd
      mkdir -p /var/lib/k0s/manifests

      #ls -al /run/secrets

      echo "Copying pki files"
      cp /run/secrets/* /var/lib/k0s/pki/
      chown root:root -R /var/lib/k0s/pki/
      chmod 600 /var/lib/k0s/pki/ca.key /var/lib/k0s/pki/*.token
      ln -s /var/lib/k0s/pki/ca.key /var/lib/k0s/pki/etcd/ca.key
      ln -s /var/lib/k0s/pki/ca.crt /var/lib/k0s/pki/etcd/ca.crt

      echo "Copying manifests"
      mkdir -p /var/lib/k0s/manifests/secrets
      cp /run/secrets/secret_* /var/lib/k0s/manifests/secrets/
      chown root:root -R /var/lib/k0s/manifests/secrets

      export ETCD_INITIAL_CLUSTER_TOKEN=k0s-etcd-cluster

      export IP_K0S1=$$(host k0s-1 | awk '{print $$4}')
      export IP_K0S2=$$(host k0s-2 | awk '{print $$4}')
      export IP_K0S3=$$(host k0s-3 | awk '{print $$4}')

      # provide dns names instead\without of ips = remote error: tls: bad certificate
      # https://github.com/etcd-io/etcd/issues/8603
      # https://etcd.io/docs/v3.3/op-guide/security/#with-peer-certificate-authentication-i-receive-certificate-is-valid-for-127001-not-my_ip
      # https://etcd.io/docs/v3.4/op-guide/clustering/
      export ETCD_INITIAL_CLUSTER="k0s-1=https://k0s-1:2380,k0s-2=https://k0s-2:2380,k0s-3=https://k0s-3:2380,k0s-1=https://$$IP_K0S1:2380,k0s-2=https://$$IP_K0S2:2380,k0s-3=https://$$IP_K0S3:2380"

      # TODO: check with etcdctl if cluster already exists
      # however node joined into cluster after restart even with value 'new'
      export ETCD_INITIAL_CLUSTER_STATE=new # existing

      if [ ! -z "$$K0S_CONFIG" ]; then
        echo "Creating /etc/k0s/config.yaml"
        mkdir -p /etc/k0s
        echo -n "$$K0S_CONFIG" | envsubst > /etc/k0s/config.yaml
      fi

      k0s controller --config=/etc/k0s/config.yaml --enable-metrics-scraper --token-file /run/secrets/controller.token #--enable-dynamic-config
    # https://docs.docker.com/compose/compose-file/compose-file-v3/#healthcheck
    # https://docs.docker.com/engine/reference/builder/#healthcheck
    healthcheck:
      test: |
        # kube-apiserver
        curl -sk https://127.0.0.1:6443 | grep -q Status || exit 1
        # konnectivity
        curl -sk http://127.0.0.1:8092 | grep -q 404 || exit 1
        # k0s
        curl -sk https://127.0.0.1:9443 | grep -q 404 || exit 1
        # kube-scheduler
        curl -sk https://127.0.0.1:10259 | grep -q Status || exit 1
        # kube-controller
        curl -sk https://127.0.0.1:10257 | grep -q Status || exit 1
        # etcd
        curl -s --cacert /var/lib/k0s/pki/etcd/ca.crt \
        --key /var/lib/k0s/pki/etcd/server.key \
        --cert /var/lib/k0s/pki/etcd/server.crt \
        https://127.0.0.1:2379/health | grep -q '"health":"true"' || exit 1

        # https://kubernetes.io/docs/reference/using-api/health-checks/
        # this command check health of whole cluster and etcd
        # in case of etcd cluster fail it returns 'livez check failed'
        # even for worked node
        #k0s kubectl get --raw='/livez?verbose' | grep -q 'livez check passed' || exit 1
      # interval in which the healthcheck process will execute
      interval: 5s
      # time duration to wait for a healthcheck
      timeout: 5s
      # define the number of tries to implement the health check after failure
      retries: 3
      # initialization time for containers that need time to bootstrap
      # it should be increased if etcd will need time to sync big amount of data
      start_period: 70s

services:
  k0s-1:
    <<: *service
    hostname: k0s-1
    container_name: k0s-1
    volumes:
      - /var/lib/k0s/etcd
      # for controllers persistent instead of volumes
      # - /data/k0s-1:/var/lib/k0s/etcd

  k0s-2:
    <<: *service
    hostname: k0s-2
    container_name: k0s-2
    volumes:
      - /var/lib/k0s/etcd
      # for controllers persistent instead of volumes
      # - /data/k0s-2:/var/lib/k0s/etcd

  k0s-3:
    <<: *service
    hostname: k0s-3
    container_name: k0s-3
    volumes:
      - /var/lib/k0s/etcd
      # for controllers persistent instead of volumes
      # - /data/k0s-3:/var/lib/k0s/etcd

  # https://www.haproxy.com/blog/haproxy-on-docker-swarm-load-balancing-and-dns-service-discovery/
  haproxy:
    image: haproxytech/haproxy-debian
    #hostname: haproxy
    #container_name: haproxy
    ports:
      - 6443:6443
    deploy:
      mode: replicated
      replicas: 2
      update_config:
        parallelism: 1
        delay: 60s
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "100m"
        max-file: "2"
    environment:
      CONFIG: |-
        global
            log          fd@2 local2
            chroot       /var/lib/haproxy
            pidfile      /var/run/haproxy.pid
            maxconn      4000
            user         haproxy
            group        haproxy
            stats socket /var/lib/haproxy/stats expose-fd listeners
            master-worker

        resolvers docker
            nameserver dns1 127.0.0.11:53
            resolve_retries 3
            timeout resolve 3s
            timeout retry   1s
            hold other      10s
            hold refused    10s
            hold nx         10s
            hold timeout    10s
            hold valid      10s
            hold obsolete   10s

        defaults
            timeout connect 10s
            timeout client 30s
            timeout server 30s
            log global
            mode http
            option httplog

        frontend  stats
            bind *:80
            default_backend stats

        backend stats
            stats enable
            stats uri /
            stats refresh 15s
            stats show-legends
            stats show-node

        frontend k0s-api
            mode tcp
            bind :6443
            default_backend k0s-api

        backend k0s-api
            mode tcp
            balance roundrobin
            server k0s-1 k0s-1:6443 check resolvers docker init-addr libc,none
            server k0s-2 k0s-2:6443 check resolvers docker init-addr libc,none
            server k0s-3 k0s-3:6443 check resolvers docker init-addr libc,none

    entrypoint: ["/bin/bash", "-c"]
    command:
    - |
      if [ ! -z "$$CONFIG" ]; then
        echo -n "$$CONFIG"  > /usr/local/etc/haproxy/haproxy.cfg
        echo >> /usr/local/etc/haproxy/haproxy.cfg
      fi
      bash /docker-entrypoint.sh haproxy -f /usr/local/etc/haproxy/haproxy.cfg
